{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeuGlqwL-pno"
      },
      "source": [
        "# Plan2Scene: Converting Floorplans to 3D Scenes\n",
        "Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa, Angel X. Chang , Manolis Savva\n",
        "\n",
        "Simon Fraser University\n",
        "\n",
        "*CVPR 2021*\n",
        "\n",
        "\n",
        "[[Project Page](https://3dlg-hcvc.github.io/plan2scene/), [GitHub](https://github.com/3dlg-hcvc/plan2scene), [Paper](https://arxiv.org/abs/2106.05375)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNpcgVKPeP5k"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/3dlg-hcvc/plan2scene/main/docs/img/intro.png' height='300'/>\n",
        "\n",
        "Our system addresses the Plan2Scene task by converting a floorplan and set of photos into a textured 3D mesh model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYoCurNIfOyU"
      },
      "source": [
        "## Plan2Scene Stages\n",
        "<img src='https://raw.githubusercontent.com/3dlg-hcvc/plan2scene/main/docs/img/task-overview.png'/>\n",
        "\n",
        "The Plan2Scene task consists of several steps: floorplan vectorization, 3D geometry construction, object placement, photo assignment, texture generation, and texture propagation.\n",
        "\n",
        "This notebook focuses on the texture generation and the texture propagation steps of the task. We assume a vectorized floorplan is available, and the photos are assigned to rooms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-dQxv2fbjxW"
      },
      "source": [
        "## Pre-requisites\n",
        "1. __Run this notebook using a GPU runtime.__\n",
        "\n",
        "2. You are required to upload a **floorplan vector of a house** (in scene.json format), **extracted rectified surface crops**, and **photo room assignments** to use this notebook. You can refer to our dataset [Rent3D++](https://3dlg-hcvc.github.io/plan2scene/) for the above information.\n",
        "\n",
        "3. To preview the final textured 3D house, you have to use [smart scene toolkit](https://github.com/smartscenes/sstk) scene viewer in your local machine. This notebook allows previewing of synthesized textures as individual images, but not as a 3D model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAO5Al75mBdV"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5Rfp1G7J_TA"
      },
      "outputs": [],
      "source": [
        "!apt-get install python3.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX-Zfc6WJ_Xm"
      },
      "outputs": [],
      "source": [
        "!apt-get update -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9mn2IaBJ_bG"
      },
      "outputs": [],
      "source": [
        "!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dmP2iIrJ_n9"
      },
      "outputs": [],
      "source": [
        "!update-alternatives --config python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED8JsaTuJ_rP"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYGnTvSBJ_v4"
      },
      "outputs": [],
      "source": [
        "!apt install python3-pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOQjaa94KhZf"
      },
      "outputs": [],
      "source": [
        "!apt install python3.7-distutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBb216x4mBgU"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngeE5VWHPxTz"
      },
      "outputs": [],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypXzvR55Qtqq"
      },
      "outputs": [],
      "source": [
        "# Download the code\n",
        "!git clone https://github.com/3dlg-hcvc/plan2scene.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBFdj-swSgcV"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGP3ALWtOAGT"
      },
      "outputs": [],
      "source": [
        "# Install torch\n",
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuBbRDNgS3LO"
      },
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "!pip install numpy pandas pyyaml orderedattrdict kornia==0.2.0 tensorboard git+https://github.com/sbrisard/moisan2011.git@master pytorch-fid==0.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVGdKvoROBZo"
      },
      "outputs": [],
      "source": [
        "# Install PyTorch Geometric\n",
        "%env CUDA=cu110\n",
        "%env TORCH=1.7.1\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --no-cache\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --no-cache # This step takes unusually long. About 30 minutes.\n",
        "# !pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --no-cache\n",
        "# !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --no-cache\n",
        "!pip install torch-geometric --no-cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TznP7y1lQxkP"
      },
      "outputs": [],
      "source": [
        "# Install noise kernel\n",
        "%cd ./plan2scene\n",
        "%cd code/src/plan2scene/texture_gen/custom_ops/noise_kernel\n",
        "!pip install .\n",
        "%cd ../../../../../../../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g699pK8thDt-"
      },
      "outputs": [],
      "source": [
        "# Download EmbarkStudios Texture Synthesis library. We use it to correct seams of textures so they can be tiled.\n",
        "%env EMBARK_TEX_SYNTH=texture-synthesis-0.8.2-x86_64-unknown-linux-musl\n",
        "!wget https://github.com/EmbarkStudios/texture-synthesis/releases/download/0.8.2/$EMBARK_TEX_SYNTH.tar.gz\n",
        "!tar -xf $EMBARK_TEX_SYNTH.tar.gz\n",
        "\n",
        "# Download seam mask\n",
        "!wget -P $EMBARK_TEX_SYNTH https://raw.githubusercontent.com/EmbarkStudios/texture-synthesis/main/imgs/masks/1_tile.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwdHxqB8hynP"
      },
      "outputs": [],
      "source": [
        "# Specify seam correction configuration\n",
        "import os\n",
        "import os.path as osp\n",
        "import json\n",
        "texture_synthesis_conf = {\n",
        "    \"texture_synthesis_path\": osp.abspath(osp.join(os.environ[\"EMBARK_TEX_SYNTH\"], \"texture-synthesis\")),\n",
        "    \"seam_mask_path\": osp.abspath(osp.join(os.environ[\"EMBARK_TEX_SYNTH\"], \"1_tile.jpg\"))\n",
        "}\n",
        "with open(\"./plan2scene/conf/plan2scene/seam_correct.json\", \"w\") as f:\n",
        "  json.dump(texture_synthesis_conf, f, indent=4)\n",
        "\n",
        "assert osp.exists(texture_synthesis_conf[\"texture_synthesis_path\"])\n",
        "assert osp.exists(texture_synthesis_conf[\"seam_mask_path\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnRdKOBjZTpY"
      },
      "source": [
        "## Load Plan2Scene Modules\n",
        "__Re-run from here if you restart the runtime.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR2VVWhUIluM"
      },
      "outputs": [],
      "source": [
        "%cd ./plan2scene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM5TW30HQ553"
      },
      "outputs": [],
      "source": [
        "!pip install kornia==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26QPwYJrRL0J"
      },
      "outputs": [],
      "source": [
        "!pip list | grep kornia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgRuHJJpYOKT"
      },
      "outputs": [],
      "source": [
        "# Load system modules\n",
        "import zipfile\n",
        "import io\n",
        "import os.path as osp\n",
        "import os\n",
        "from google.colab import files\n",
        "import json\n",
        "import pandas as pd\n",
        "import sys\n",
        "from IPython.core.display import display, HTML\n",
        "from ipywidgets import interact\n",
        "from PIL import ImageDraw, Image\n",
        "import torch\n",
        "\n",
        "# Load Plan2Scene modules\n",
        "sys.path.append(\"./code/src\")\n",
        "from arch_parser.parser import parse_arch_json, parse_object_jsons, PreferredFormat, parse_scene_json_from_file\n",
        "from plan2scene.common.residence import House\n",
        "from plan2scene.config_manager import ConfigManager\n",
        "from plan2scene.common.image_description import ImageDescription, ImageSource\n",
        "from plan2scene.utils.io import load_image\n",
        "from arch_parser.json_util import pil_to_data_url\n",
        "# from plan2scene.texture_gen.predictor import TextureGenPredictor\n",
        "# from plan2scene.texture_gen.utils.io import load_conf_eval\n",
        "# from plan2scene.crop_select.util import fill_textures\n",
        "# from plan2scene.texture_prop.utils import update_embeddings\n",
        "# from plan2scene.texture_prop.predictor import TexturePropPredictor\n",
        "# from plan2scene.texture_prop.graph_generators import InferenceHGG\n",
        "# from plan2scene.texture_prop.houses_dataset import HouseDataset\n",
        "from torch_geometric.data import DataLoader\n",
        "from plan2scene.common.house_parser import save_arch\n",
        "from plan2scene.utils.tile_util import tile_image\n",
        "\n",
        "# Initialize config manager\n",
        "conf = ConfigManager()\n",
        "conf.load_default_args()\n",
        "conf.texture_gen.texture_synth_conf = \"./conf/plan2scene/texture_synth_conf/v2.yml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytcANMEDbX0s"
      },
      "outputs": [],
      "source": [
        "def upload():\n",
        "  \"\"\"\n",
        "  Utility method for file uploads.\n",
        "  \"\"\"\n",
        "  uploaded = files.upload()\n",
        "  assert len(uploaded) == 1\n",
        "  content = None\n",
        "  for key in uploaded.keys():\n",
        "    print('Uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=key, length=len(uploaded[key])))\n",
        "    return uploaded[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiA4i6MsX6Bw"
      },
      "source": [
        "## Data\n",
        "In this section, we will obtain the following files from you.\n",
        "1. A floorplan vector in the scene.json format. Refer to the Rent3D++ dataset for examples. The scene.json format is [explained here](https://github.com/3dlg-hcvc/plan2scene/blob/main/docs/md/scene_json_format.md).\n",
        "2. Rectified surface crops extracted from photos of the house.\n",
        "  - If you wish to extract new rectified surface crops, [refer to instructions here](https://github.com/3dlg-hcvc/plan2scene/blob/main/docs/md/extract_crops.md).\n",
        "  - Otherwise, you can provide rectified surface crops provided with the Rent3D++ dataset.\n",
        "3. Photo room assignments specified in a photoroom.csv file. Refer to the Rent3D++ dataset for examples. The format of the photoroom.csv file is [described here](https://github.com/3dlg-hcvc/plan2scene/blob/main/docs/md/rent3dpp_data_organization.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLPwp19DYEOq"
      },
      "source": [
        "__Task:__ Upload a scene.json file describing a floorplan vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2ERdfn6SVjy"
      },
      "outputs": [],
      "source": [
        "house_key = None\n",
        "if not osp.exists(\"./data/processed/archs\"):\n",
        "  os.makedirs(\"./data/processed/archs\")\n",
        "\n",
        "scene_json_content = upload()\n",
        "scene_json = json.loads(scene_json_content.decode())\n",
        "scene_id = scene_json[\"scene\"][\"arch\"][\"id\"]\n",
        "\n",
        "with open(f\"./data/processed/archs/{scene_id}.scene.json\", \"w\") as f:\n",
        "  f.write(scene_json_content.decode())\n",
        "\n",
        "print(\"Sketch of floorplan vector.\")\n",
        "arch_house = parse_scene_json_from_file(f\"./data/processed/archs/{scene_id}.scene.json\",  None)\n",
        "arch_house.sketch_house()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Dp_CcMeaMx"
      },
      "source": [
        "__Task:__ Upload rectified surface crops extracted from photos.\n",
        "\n",
        "The rectified surface crops should be named in the format '[photo_file_name_without_extension]_[surface_id]_crop[crop_id].png'. The rectified surface crops provided with the Rent3D++ dataset are in this format. If you extract new rectified surface crops using the code we provide, they will also be in this format.\n",
        "\n",
        "When you run the next cell, you will be prompted to upload 3 zip files, one per surface type, containing crops sampled from rectified surfaces.\n",
        "\n",
        "For the Rent3D++ dataset, you can create such zip files using the rectified crops available in the './processed/rectified_crops/[surface_type]' directories provided by the Rent3D++ dataset. Note that each photo of the Rent3D++ dataset has a filename starting with the house_key. So, you can search and identify the crops belonging to the scene.json file you uploaded before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU2-nxcNY9xw"
      },
      "outputs": [],
      "source": [
        "for surface in conf.surfaces:\n",
        "  print(f\"Please provide a .zip file containing rectified crops extracted from {surface} surfaces.\")\n",
        "\n",
        "  if not osp.exists(f\"./data/processed/rectified_crops/{surface}\"):\n",
        "    os.makedirs(f\"./data/processed/rectified_crops/{surface}\")\n",
        "\n",
        "  crops_content = upload()\n",
        "  zf = zipfile.ZipFile(io.BytesIO(crops_content), \"r\")\n",
        "  zf.extractall(path=f\"./data/processed/rectified_crops/{surface}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2eo8kbdigi3"
      },
      "source": [
        "__Task:__ Upload a photoroom.csv file describing the assignment of photos to rooms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_G2bZwsf_dJ"
      },
      "outputs": [],
      "source": [
        "photoroom_content = upload()\n",
        "\n",
        "os.makedirs(\"./data/processed/photo_assignments\", exist_ok=True)\n",
        "\n",
        "photo_room_df = pd.read_csv(io.BytesIO(photoroom_content))\n",
        "assert \"roomId\" in photo_room_df.columns\n",
        "assert \"photo\" in photo_room_df.columns\n",
        "photo_room_df.to_csv(f\"./data/processed/photo_assignments/{scene_id}.photoroom.csv\")\n",
        "photo_room_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GveKRnugfDY"
      },
      "outputs": [],
      "source": [
        "# Re-load house with crop assignments\n",
        "arch_house = parse_scene_json_from_file(f\"./data/processed/archs/{scene_id}.scene.json\", f\"./data/processed/photo_assignments/{scene_id}.photoroom.csv\")\n",
        "house = House.from_arch_house(arch_house, surfaces=conf.surfaces)\n",
        "room_count = len(house.rooms)\n",
        "\n",
        "for room_index, room in house.rooms.items():\n",
        "    for photo in room.photos:\n",
        "        for surface in conf.surfaces:\n",
        "            surface_instances = [i for i in range(conf.texture_gen.masks_per_surface[surface])]\n",
        "            for surface_instance in surface_instances:\n",
        "                for crop_instance in range(conf.texture_gen.crops_per_mask):\n",
        "                    candidate_key = \"%s_%d_crop%d\" % (photo, surface_instance, crop_instance)\n",
        "                    if osp.exists(osp.join(conf.data_paths.rectified_crops_path, surface, candidate_key + \".png\")):\n",
        "                        image = load_image(\n",
        "                            osp.join(conf.data_paths.rectified_crops_path, surface, candidate_key + \".png\"))\n",
        "                        room.surface_textures[surface][candidate_key] = ImageDescription(image, ImageSource.NEURAL_SYNTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5kgQYqlWb8P"
      },
      "source": [
        "__Task:__ Let's preview the data you have provided.\n",
        "\n",
        "Run the below cell. Then select a room_id from the dropbox to preview crops assigned to a room."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUuitcX5WfPN"
      },
      "outputs": [],
      "source": [
        "room_ids = list(house.rooms)\n",
        "\n",
        "@interact(room_id=room_ids)\n",
        "def preview_room_crops(room_id):\n",
        "  room = house.rooms[room_id]\n",
        "\n",
        "  html_buffer = io.StringIO()\n",
        "  html_buffer.write(\"<table>\\n\")\n",
        "\n",
        "  for i, surface in enumerate(conf.surfaces):\n",
        "    html_buffer.write(\"<tr>\\n\")\n",
        "    if i == 0:\n",
        "      # Left section\n",
        "      html_buffer.write(\"<td rowspan='{colspan}'>\\n\".format( colspan=len(conf.surfaces)))\n",
        "      html_buffer.write(\"<img src='{src}'/>\\n\".format(src=pil_to_data_url(house.sketch_house(focused_room_id=room.room_id))))\n",
        "      html_buffer.write(\"<br/>\\n\")\n",
        "      html_buffer.write(\"<p style='text-align:center;'><b>Room Types: </b>{types}</p>\\n\".format(types=str(room.types)))\n",
        "      html_buffer.write(\"</td>\\n\")\n",
        "\n",
        "    # Right section\n",
        "    html_buffer.write(\"<td>\\n\")\n",
        "    html_buffer.write(f\"<span>{surface} crops</span>\\n\")\n",
        "    html_buffer.write(\"<br/>\\n\")\n",
        "    for crop_id, crop in room.surface_textures[surface].items():\n",
        "      html_buffer.write(\"<img src='{src}'/>\\n\".format(src=pil_to_data_url(crop.image)))\n",
        "    html_buffer.write(\"</td>\\n\")\n",
        "    html_buffer.write(\"</tr>\\n\")\n",
        "\n",
        "  html_buffer.write(\"</table>\\n\")\n",
        "  display(HTML(html_buffer.getvalue()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54deSSG3oozH"
      },
      "source": [
        "# Synthesize textures for observed surfaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkrA3eP4o7yW"
      },
      "source": [
        "Download pre-trained weights for the texture synthesis stage. We use the pre-trained model trained on stationary textures dataset version 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9J02fqzgQlo"
      },
      "outputs": [],
      "source": [
        "%env SYNTH_CHECKPOINT_NAME=loss-7.67493-epoch-750.ckpt\n",
        "!mkdir ./trained_models/texture_gen/v2/checkpoints\n",
        "!wget -P ./trained_models/texture_gen/v2/checkpoints http://aspis.cmpt.sfu.ca/projects/plan2scene/pretrained-models/texture-synth/v2/checkpoints/$SYNTH_CHECKPOINT_NAME\n",
        "conf.texture_gen.checkpoint_path = \"./trained_models/texture_gen/v2/checkpoints/{checkpoint_name}\".format(checkpoint_name = os.environ[\"SYNTH_CHECKPOINT_NAME\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRXSfiqOgQ-w"
      },
      "outputs": [],
      "source": [
        "# Load texture synthesis network\n",
        "tg_predictor = TextureGenPredictor(\n",
        "        conf=load_conf_eval(config_path=conf.texture_gen.texture_synth_conf),\n",
        "        rgb_median_emb=conf.texture_gen.rgb_median_emb)\n",
        "tg_predictor.load_checkpoint(checkpoint_path=conf.texture_gen.checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOhn9OGxqB2P"
      },
      "outputs": [],
      "source": [
        "# Compute texture embeddings for observed surfaces (Code adapted from ./code/scripts/preprocessing/fill_room_embeddigs.py)\n",
        "for room_index, room in house.rooms.items():\n",
        "    for surface in conf.surfaces:\n",
        "      for candidate_key, image_description in room.surface_textures[surface].items():\n",
        "        image = image_description.image\n",
        "        emb, loss = tg_predictor.predict_embs([image])\n",
        "        room.surface_embeddings[surface][candidate_key] = emb\n",
        "        room.surface_losses[surface][candidate_key] = loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEby3IPQrdV7"
      },
      "outputs": [],
      "source": [
        "# Synthesize textures for observed surfaces using the most suitable crop identified by VGG textureness score.\n",
        "# Code copied from ./code/scripts/crop_select/vgg_crop_selector.py)\n",
        "\n",
        "def get_least_key(kv):\n",
        "    \"\"\"\n",
        "    Given a dictionary, returns the key with minimum value.\n",
        "    :param kv: Dictionary considered.\n",
        "    :return: Key with the minimum value.\n",
        "    \"\"\"\n",
        "    min_k = None\n",
        "    min_v = None\n",
        "    for k, v in kv.items():\n",
        "        if min_v is None or v.item() < min_v:\n",
        "            min_k = k\n",
        "            min_v = v.item()\n",
        "\n",
        "    return min_k\n",
        "\n",
        "for room_index, room in house.rooms.items():\n",
        "    # Calculate the least VGG loss embeddings\n",
        "    for surface in room.surface_embeddings:\n",
        "        least_key = get_least_key(room.surface_losses[surface])\n",
        "        if least_key is not None:\n",
        "            room.surface_embeddings[surface] = {\"prop\": room.surface_embeddings[surface][least_key]}\n",
        "            room.surface_losses[surface] = {\"prop\": room.surface_losses[surface][least_key]}\n",
        "        else:\n",
        "            room.surface_embeddings[surface] = {}\n",
        "            room.surface_losses[surface] = {}\n",
        "\n",
        "fill_textures(conf, {house.house_key: house}, predictor=tg_predictor, log=False, image_source=ImageSource.VGG_CROP_SELECT, skip_existing_textures=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiDoz5LvukW3"
      },
      "source": [
        "# Propagate Textures to Unobserved Surfaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEmIMXCFuYoG"
      },
      "outputs": [],
      "source": [
        "# Download pre-trained weights for the GNN.\n",
        "%env PROP_CHECKPOINT_NAME=loss-0.51442-epoch-250.ckpt\n",
        "!mkdir ./trained_models/texture_prop/v2/checkpoints\n",
        "!wget -P ./trained_models/texture_prop/v2/checkpoints https://aspis.cmpt.sfu.ca/projects/plan2scene/pretrained-models/texture-prop/synth-v2-epoch750/checkpoints/$PROP_CHECKPOINT_NAME\n",
        "\n",
        "prop_checkpoint_path = \"./trained_models/texture_prop/v2/checkpoints/{checkpoint_name}\".format(checkpoint_name = os.environ[\"PROP_CHECKPOINT_NAME\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWDb8cYbvkcA"
      },
      "outputs": [],
      "source": [
        "# Load GNN and graph generator\n",
        "# Code copied from ./code/scripts/texture_prop/gnn_texture_prop.py\n",
        "tp_predictor = TexturePropPredictor(conf, conf.texture_prop)\n",
        "tp_predictor.load_checkpoint(checkpoint_path=prop_checkpoint_path)\n",
        "nt_graph_generator = InferenceHGG(conf=conf, include_target=False)\n",
        "\n",
        "# Graph dataset\n",
        "houses = {scene_id: house}\n",
        "\n",
        "nt_dataset = HouseDataset(houses, graph_generator=nt_graph_generator)\n",
        "nt_dataloader = DataLoader(nt_dataset, batch_size=conf.texture_prop.train.bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0IW4WIDybFv"
      },
      "outputs": [],
      "source": [
        "# Graph inference\n",
        "# Code copied from ./code/scripts/texture_prop/gnn_texture_prop.py\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(nt_dataloader):\n",
        "        print(\"Batch [%d/%d] Graph Inference\" % (i, len(nt_dataloader)))\n",
        "        output = tp_predictor.predict(batch.to(conf.texture_prop.device))\n",
        "        update_embeddings(conf, houses, batch, output,\n",
        "                          keep_existing_predictions=True)\n",
        "\n",
        "fill_textures(conf, houses, log=True, predictor=tg_predictor, image_source=ImageSource.GNN_PROP, skip_existing_textures=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhfa8_5Gz6c4"
      },
      "source": [
        "# Correct seams of synthesized textures\n",
        "We correct seams of synthesized textures, making them tileable textures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO0ROxQ6jz1x"
      },
      "outputs": [],
      "source": [
        "embark_texture_synthesis_path = conf.seam_correct_config.texture_synthesis_path\n",
        "seam_mask_path = conf.seam_correct_config.seam_mask_path\n",
        "\n",
        "for room_index, room in house.rooms.items():\n",
        "  print(f\"Processing {room_index}/{room_count}\")\n",
        "  for surface in room.surface_textures:\n",
        "      texture_description = room.surface_textures[surface][\"prop\"]\n",
        "      assert isinstance(texture_description, ImageDescription)\n",
        "      texture = texture_description.image\n",
        "      texture = tile_image(texture, embark_texture_synthesis_path, seam_mask_path)\n",
        "      texture_description.image = texture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CyT4nzd2JD3"
      },
      "source": [
        "## Preview Synthesized Textures\n",
        "\n",
        "Run the below cell to preview the texture crops synthesized for each surface. Use the drop-down to select a room.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9llYb4Y3Ouw"
      },
      "outputs": [],
      "source": [
        "@interact(room_id=room_ids)\n",
        "def preview_room_textures(room_id):\n",
        "  room = house.rooms[room_id]\n",
        "\n",
        "  html_buffer = io.StringIO()\n",
        "  html_buffer.write(\"<table>\\n\")\n",
        "\n",
        "  for i, surface in enumerate(conf.surfaces):\n",
        "    html_buffer.write(\"<tr>\\n\")\n",
        "    if i == 0:\n",
        "      # Left section\n",
        "      html_buffer.write(\"<td rowspan='{colspan}'>\\n\".format( colspan=len(conf.surfaces)))\n",
        "      html_buffer.write(\"<img src='{src}'/>\\n\".format(src=pil_to_data_url(house.sketch_house(focused_room_id=room.room_id))))\n",
        "      html_buffer.write(\"<br/>\\n\")\n",
        "      html_buffer.write(\"<p style='text-align:center;'><b>{types}</b></p>\\n\".format(types=str(room.types)))\n",
        "      html_buffer.write(\"</td>\\n\")\n",
        "\n",
        "    # Right section\n",
        "    html_buffer.write(\"<td><span>{surface}</span><br/><img src='{src}'/></td>\\n\".format(src=pil_to_data_url(room.surface_textures[surface][\"prop\"].image), surface=surface))\n",
        "    html_buffer.write(\"</tr>\\n\")\n",
        "\n",
        "  html_buffer.write(\"</table>\\n\")\n",
        "  display(HTML(html_buffer.getvalue()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-47WCtin0E9c"
      },
      "source": [
        "## Output Model of House\n",
        "You can download the textured scene.json file by running the cell below. Open it using the [smart scene toolkit](https://github.com/smartscenes/sstk) scene viewer to view the 3D model of the house. You have to setup the scene viewer in your local machine to preview it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFL7Qdvc0Bqb"
      },
      "outputs": [],
      "source": [
        "if not osp.exists(\"./outputs\"):\n",
        "  os.makedirs(\"./outputs\")\n",
        "save_arch(conf, house, osp.join(\"./outputs/\", scene_id), texture_both_sides_of_walls = True)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(osp.join(\"./outputs/\", scene_id + \".scene.json\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
