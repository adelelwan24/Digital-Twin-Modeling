# Model 1: Stable Dream Fusion

The model uses the Zero 1-to-3 framework to generate 3d models based on a single RGB image input. The pre-trained model works by using a large diffusion model that is able to leverage zero-shot 3D shape reconstructions, this could be performed through

## Attributes

**Pre-trained Model:** Zero123 **Fine Tuning:** Available

**Dataset:[ NeRF](https://paperswithcode.com/dataset/nerf)**, [Mip-NeRF 360](https://paperswithcode.com/dataset/mip-nerf-360), [T$^3$Bench ](https://paperswithcode.com/dataset/t-3-bench)**Evaluation:** Available: R-precision evaluation

**Output extensions**: obj

**Open source:** Yes

**Training:** supports


## Trials:

1. 10 iterations
   1) Result: poor quality, took 30 minutes
1. 100 iterations: Runtime error, ran out of GPU

# Model 2: InstantMesh

It is a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multi-view diffusion model and a sparse- view reconstruction model based on the LRM architecture.

Instant3D proposes a diagram that predicts 3D shapes via an enhanced LRM with multi-view input generated by diffusion models. The method merges the LRMs with image generation models, which significantly improves the generalization ability. LRM-based methods use tri-planes as the 3D representation, where novel views are synthesized using an MLP, multilayer perceptron .

## Attributes

**Pre-trained Model:** LRM Base, LRM Large, nerf base, nerf Large **Fine Tuning:** Available (Zero123 only)

**Dataset:[ OmniObject3D](https://paperswithcode.com/dataset/omniobject3d)**, [Objaverse](https://paperswithcode.com/dataset/objaverse)

**Evaluation:** Not available **Output extensions:** obj **Open source:** Yes **Texture map**: available


## Trials:

1. LRM, Base

a) Results: 1 minute per each image

` `![](imgs/Aspose.Words.12e9945b-adfd-4fe7-97f8-d2770063434b.001.png) ![](imgs/Aspose.Words.12e9945b-adfd-4fe7-97f8-d2770063434b.002.png)![](imgs/Aspose.Words.12e9945b-adfd-4fe7-97f8-d2770063434b.003.png)

2. LRM, large

a) Ran out of memory (15 gb VRAM required), however, it outputs the result on two stages, images sample stage, and obj stage

# Model 3: TripoSR

TripoSR leverages the transformer architecture and is specifically designed for single- image 3D

reconstruction. It takes a single RGB image as input and outputs a 3D representation of the object in the image. The core of TripoSR includes components: an image encoder, an image-to-triplane decoder, and a triplane-based neural radiance field (NeRF). The image encoder is initialized with a pre-trained vision transformer model, DINOv1, which projects an RGB image into a set of latent vectors. These vectors encode the global and local features of the image and include the necessary information to reconstruct the 3D object.

The subsequent image-to-triplane decoder transforms the latent vectors onto the triplane-NeRF representation. The triplane-NeRF representation is a compact and expressive 3D representation, well-suited for representing objects with complex shapes and textures.

## Attributes

**Pre-trained Model:** Implemented LRM model **Dataset:[ OmniObject3D](https://paperswithcode.com/dataset/omniobject3d)**, [Objaverse](https://paperswithcode.com/dataset/objaverse)

**Evaluation:** Not available **Output extensions:** obj, glb **Open source:** Yes

**Texture map**: available

**Trials:**

30-45 seconds each

![](imgs/Aspose.Words.12e9945b-adfd-4fe7-97f8-d2770063434b.004.png)** ![](imgs/Aspose.Words.12e9945b-adfd-4fe7-97f8-d2770063434b.005.png)![](imgs/Aspose.Words.12e9945b-adfd-4fe7-97f8-d2770063434b.006.png)
